# Import necessary libraries
import pandas as pd
import numpy as np
import os
import time
import json
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, IsolationForest
from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score
from sklearn.neural_network import MLPClassifier
from sklearn.svm import OneClassSVM
from sklearn.cluster import DBSCAN
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, LSTM, Input, Dropout, Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from river import anomaly, compose, preprocessing, drift
import warnings
warnings.filterwarnings('ignore')

class CyberAttackDetector:
    def __init__(self, config_path="config.json"):
        """
        Initialize the cyber attack detector with configuration settings
        """
        # Load configuration
        with open(config_path, 'r') as config_file:
            self.config = json.load(config_file)
        
        # Set up directories
        self.model_dir = self.config.get('model_dir', 'models')
        self.data_dir = self.config.get('data_dir', 'data')
        os.makedirs(self.model_dir, exist_ok=True)
        os.makedirs(self.data_dir, exist_ok=True)
        
        # Initialize model components
        self.supervised_models = {}
        self.anomaly_models = {}
        self.sequence_models = {}
        self.preprocessors = {}
        self.feature_cols = []
        self.categorical_cols = []
        self.numerical_cols = []
        self.target_col = self.config.get('target_column', 'attack_type')
        
        # Concept drift detection
        self.drift_detector = drift.ADWIN()
        
        # Active learning parameters
        self.uncertainty_threshold = self.config.get('uncertainty_threshold', 0.3)
        
        # Ensemble weights
        self.ensemble_weights = {
            'supervised': 0.6,
            'anomaly': 0.3,
            'sequence': 0.1
        }
        
        # Attack knowledge base
        self.attack_signatures = {}
        
        # Initialize logging
        self.setup_logging()
        
        # Track model performance over time
        self.performance_history = {
            'timestamp': [],
            'f1_score': [],
            'precision': [],
            'recall': [],
            'false_positive_rate': []
        }
    
    def setup_logging(self):
        """Set up logging for the detector"""
        import logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f"{self.model_dir}/detector.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('CyberAttackDetector')
        self.logger.info("Cyber Attack Detector initialized")
    
    def load_data(self, data_path=None):
        """
        Load data from CSV files or other sources
        """
        if data_path is None:
            data_path = os.path.join(self.data_dir, self.config.get('training_data', 'training_data.csv'))
        
        self.logger.info(f"Loading data from {data_path}")
        
        try:
            data = pd.read_csv(data_path)
            self.logger.info(f"Data loaded successfully. Shape: {data.shape}")
            return data
        except Exception as e:
            self.logger.error(f"Error loading data: {str(e)}")
            raise
    
    def preprocess_data(self, data):
        """
        Preprocess the data for training
        """
        self.logger.info("Preprocessing data")
        
        # Identify feature columns
        if not self.feature_cols:
            # Exclude target column and any metadata columns
            exclude_cols = [self.target_col] + self.config.get('metadata_columns', [])
            self.feature_cols = [col for col in data.columns if col not in exclude_cols]
        
        # Identify categorical and numerical columns
        if not self.categorical_cols:
            self.categorical_cols = [col for col in self.feature_cols 
                                   if data[col].dtype == 'object' or data[col].nunique() < 10]
            self.numerical_cols = [col for col in self.feature_cols 
                                 if col not in self.categorical_cols]
        
        # Create preprocessing pipeline
        numeric_transformer = Pipeline(steps=[
            ('scaler', StandardScaler())
        ])
        
        categorical_transformer = Pipeline(steps=[
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ])
        
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, self.numerical_cols),
                ('cat', categorical_transformer, self.categorical_cols)
            ])
        
        # Store preprocessor
        self.preprocessors['main'] = preprocessor
        
        # Split data into features and target
        X = data[self.feature_cols]
        y = data[self.target_col] if self.target_col in data.columns else None
        
        # Fit preprocessor if this is training data
        if y is not None:
            X_transformed = preprocessor.fit_transform(X)
        else:
            X_transformed = preprocessor.transform(X)
        
        self.logger.info(f"Data preprocessing complete. Shape after transformation: {X_transformed.shape}")
        
        return X_transformed, y
    
    def train_supervised_models(self, X, y):
        """
        Train the supervised learning models
        """
        self.logger.info("Training supervised models")
        
        # Create and train Random Forest model
        rf_model = RandomForestClassifier(
            n_estimators=self.config.get('rf_n_estimators', 100),
            max_depth=self.config.get('rf_max_depth', None),
            random_state=42
        )
        rf_model.fit(X, y)
        self.supervised_models['random_forest'] = rf_model
        
        # Create and train Gradient Boosting model
        gb_model = GradientBoostingClassifier(
            n_estimators=self.config.get('gb_n_estimators', 100),
            learning_rate=self.config.get('gb_learning_rate', 0.1),
            max_depth=self.config.get('gb_max_depth', 3),
            random_state=42
        )
        gb_model.fit(X, y)
        self.supervised_models['gradient_boosting'] = gb_model
        
        # Create and train Neural Network model
        nn_model = MLPClassifier(
            hidden_layer_sizes=self.config.get('nn_hidden_layers', (100, 50)),
            activation=self.config.get('nn_activation', 'relu'),
            solver=self.config.get('nn_solver', 'adam'),
            random_state=42,
            max_iter=self.config.get('nn_max_iter', 300)
        )
        nn_model.fit(X, y)
        self.supervised_models['neural_network'] = nn_model
        
        self.logger.info("Supervised models trained successfully")
    
    def train_anomaly_models(self, X):
        """
        Train the anomaly detection models
        """
        self.logger.info("Training anomaly detection models")
        
        # Isolation Forest for anomaly detection
        iso_model = IsolationForest(
            n_estimators=self.config.get('iso_n_estimators', 100),
            contamination=self.config.get('iso_contamination', 'auto'),
            random_state=42
        )
        iso_model.fit(X)
        self.anomaly_models['isolation_forest'] = iso_model
        
        # One-class SVM for anomaly detection
        svm_model = OneClassSVM(
            nu=self.config.get('svm_nu', 0.1),
            kernel=self.config.get('svm_kernel', 'rbf'),
            gamma=self.config.get('svm_gamma', 'scale')
        )
        svm_model.fit(X)
        self.anomaly_models['one_class_svm'] = svm_model
        
        # If we have enough data, train an autoencoder
        if X.shape[0] >= 1000:
            self.train_autoencoder(X)
        
        self.logger.info("Anomaly detection models trained successfully")
    
    def train_autoencoder(self, X):
        """
        Train an autoencoder for anomaly detection
        """
        self.logger.info("Training autoencoder for anomaly detection")
        
        # Convert to dense if sparse
        if isinstance(X, np.ndarray):
            input_dim = X.shape[1]
        else:  # Assuming sparse matrix
            input_dim = X.shape[1]
            X = X.toarray()
        
        # Define the autoencoder architecture
        input_layer = Input(shape=(input_dim,))
        
        # Encoder
        encoded = Dense(128, activation='relu')(input_layer)
        encoded = Dense(64, activation='relu')(encoded)
        encoded = Dense(32, activation='relu')(encoded)
        
        # Decoder
        decoded = Dense(64, activation='relu')(encoded)
        decoded = Dense(128, activation='relu')(decoded)
        decoded = Dense(input_dim, activation='sigmoid')(decoded)
        
        # Autoencoder model
        autoencoder = Model(input_layer, decoded)
        autoencoder.compile(optimizer='adam', loss='mean_squared_error')
        
        # Train the autoencoder
        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
        
        history = autoencoder.fit(
            X, X,
            epochs=self.config.get('autoencoder_epochs', 50),
            batch_size=self.config.get('autoencoder_batch_size', 32),
            shuffle=True,
            validation_split=0.2,
            callbacks=[early_stopping]
        )
        
        # Save the trained autoencoder
        self.anomaly_models['autoencoder'] = autoencoder
        
        self.logger.info("Autoencoder trained successfully")
    
    def train_sequence_model(self, X, y=None):
        """
        Train sequence models for temporal pattern detection
        Assumes X is a 3D array with shape (samples, time_steps, features)
        """
        self.logger.info("Preparing sequence data")
        
        # If X is not already in a 3D format, reshape it
        # This is a simplified approach - in practice, you'd need to create proper sequences
        if len(X.shape) == 2:
            # Convert to dense if sparse
            if not isinstance(X, np.ndarray):
                X = X.toarray()
            
            # For demonstration, use a simple reshaping
            # In practice, you would create proper time series sequences
            sequence_length = self.config.get('sequence_length', 10)
            if X.shape[0] > sequence_length:
                # This is a simplified approach for demonstration
                samples = X.shape[0] - sequence_length + 1
                sequences = np.zeros((samples, sequence_length, X.shape[1]))
                
                for i in range(samples):
                    sequences[i] = X[i:i+sequence_length]
                
                X_seq = sequences
                if y is not None:
                    # Take the label of the last timestep in each sequence
                    y_seq = y[sequence_length-1:]
                else:
                    y_seq = None
            else:
                self.logger.warning("Not enough data for sequence modeling")
                return
        else:
            X_seq = X
            y_seq = y
        
        self.logger.info(f"Training sequence model with data shape: {X_seq.shape}")
        
        # Create and train LSTM model
        model = Sequential()
        model.add(LSTM(64, input_shape=(X_seq.shape[1], X_seq.shape[2]), return_sequences=True))
        model.add(LSTM(32))
        model.add(Dense(16, activation='relu'))
        
        if y_seq is not None:
            # For classification, use an appropriate output layer
            n_classes = len(np.unique(y_seq))
            if n_classes > 2:
                model.add(Dense(n_classes, activation='softmax'))
                loss = 'sparse_categorical_crossentropy'
            else:
                model.add(Dense(1, activation='sigmoid'))
                loss = 'binary_crossentropy'
            
            model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])
            
            # Train the model
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
            history = model.fit(
                X_seq, y_seq,
                epochs=self.config.get('lstm_epochs', 50),
                batch_size=self.config.get('lstm_batch_size', 32),
                validation_split=0.2,
                callbacks=[early_stopping]
            )
        else:
            # For sequence anomaly detection, use reconstruction
            model.add(Dense(X_seq.shape[2], activation='linear'))
            model.compile(optimizer='adam', loss='mse')
            
            # Train the model
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
            history = model.fit(
                X_seq, X_seq,
                epochs=self.config.get('lstm_epochs', 50),
                batch_size=self.config.get('lstm_batch_size', 32),
                validation_split=0.2,
                callbacks=[early_stopping]
            )
        
        self.sequence_models['lstm'] = model
        self.logger.info("Sequence model trained successfully")
    
    def train(self, data=None):
        """
        Main training method
        """
        self.logger.info("Starting model training")
        
        if data is None:
            data = self.load_data()
        
        # Preprocess data
        X, y = self.preprocess_data(data)
        
        # Split data for training and evaluation
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Train supervised models
        self.train_supervised_models(X_train, y_train)
        
        # Train anomaly detection models
        self.train_anomaly_models(X_train)
        
        # Train sequence models if applicable
        # Note: This would require appropriate sequence data
        # self.train_sequence_model(X_train, y_train)
        
        # Evaluate models
        self.evaluate(X_test, y_test)
        
        # Save models
        self.save_models()
        
        self.logger.info("Model training complete")
    
    def evaluate(self, X, y):
        """
        Evaluate model performance
        """
        self.logger.info("Evaluating model performance")
        
        results = {}
        for name, model in self.supervised_models.items():
            y_pred = model.predict(X)
            f1 = f1_score(y, y_pred, average='weighted')
            precision = precision_score(y, y_pred, average='weighted')
            recall = recall_score(y, y_pred, average='weighted')
            
            results[name] = {
                'f1_score': f1,
                'precision': precision,
                'recall': recall
            }
            
            self.logger.info(f"{name} - F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}")
            
        self.performance_history['timestamp'].append(datetime.now().isoformat())
        self.performance_history['f1_score'].append(results['random_forest']['f1_score'])
        self.performance_history['precision'].append(results['random_forest']['precision'])
        self.performance_history['recall'].append(results['random_forest']['recall'])
        
        return results
    
    def detect_concept_drift(self, X):
        """
        Detect concept drift in real-time using ADWIN
        """
        drift_detected = False
        for x in X:
            self.drift_detector.update(x.mean())
            if self.drift_detector.change_detected:
                self.logger.warning("Concept drift detected! Retraining model...")
                drift_detected = True
                break
        return drift_detected

    def update_model(self, new_data):
        """
        Retrain model with new attack patterns when concept drift is detected
        """
        self.logger.info("Updating model with new attack data")
        X_new, y_new = self.preprocess_data(new_data)
        self.train_supervised_models(X_new, y_new)
        self.train_anomaly_models(X_new)
        self.logger.info("Model retraining complete")
    
    def detect_attack(self, X):
        """
        Detect cyber attacks in real-time
        """
        self.logger.info("Running real-time attack detection")
        predictions = []
        
        for name, model in self.supervised_models.items():
            pred = model.predict(X)
            predictions.append(pred)
        
        anomaly_scores = [self.anomaly_models['isolation_forest'].decision_function(X)]
        attack_detected = np.mean(predictions) > 0.5 or np.mean(anomaly_scores) < -0.5
        
        if attack_detected:
            self.logger.warning("Potential cyber attack detected!")
        return attack_detected
    
    def save_models(self):
        """
        Save trained models for future use
        """
        for name, model in self.supervised_models.items():
            joblib.dump(model, os.path.join(self.model_dir, f"{name}.pkl"))
        
        for name, model in self.anomaly_models.items():
            joblib.dump(model, os.path.join(self.model_dir, f"{name}.pkl"))
        
        self.logger.info("All models saved successfully")
    
    def load_models(self):
        """
        Load trained models
        """
        for name in ['random_forest', 'gradient_boosting', 'neural_network']:
            model_path = os.path.join(self.model_dir, f"{name}.pkl")
            if os.path.exists(model_path):
                self.supervised_models[name] = joblib.load(model_path)
        
        for name in ['isolation_forest', 'one_class_svm']:
            model_path = os.path.join(self.model_dir, f"{name}.pkl")
            if os.path.exists(model_path):
                self.anomaly_models[name] = joblib.load(model_path)
        
        self.logger.info("All models loaded successfully")
